{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f406ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import boto3\n",
    "import itertools\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import uuid\n",
    "from io import StringIO\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# suppress irrelevant pandas warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='pandas only supports SQLAlchemy connectable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7ba150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.connect()\n",
    "    \n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=self.config.host,\n",
    "                database=self.config.database,\n",
    "                user=self.config.user,\n",
    "                password=self.config.password,\n",
    "                port=self.config.port\n",
    "            )\n",
    "            print(\"Connection to RDS successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to RDS: {e}\")\n",
    "            \n",
    "    def data(self, query, limit=100, offset=0):\n",
    "        df = None\n",
    "        try:\n",
    "            query = f\"SELECT * FROM ({query}) LIMIT {limit} OFFSET {offset}\"\n",
    "            df = pd.read_sql(query, self.connection)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying data: {e}\")\n",
    "        return df\n",
    "    \n",
    "    def count(self, query):\n",
    "        total_count = None\n",
    "        try:\n",
    "            count_query = f\"SELECT COUNT(*) FROM ({query}) AS count_query\"\n",
    "            result = pd.read_sql(count_query, self.connection)\n",
    "            total_count = result.iloc[0, 0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying count: {e}\")\n",
    "        return total_count\n",
    "\n",
    "    def enrollment_query(self):\n",
    "        query = \"\"\"\n",
    "        SELECT e.EnrollmentID, e.PersonalID, e.ProjectId, e.EntryDate, e.DateOfEngagement,\n",
    "            CASE \n",
    "                WHEN e.LivingSituation = 116 THEN 'Place Not Meant For Habitation'\n",
    "                WHEN e.LivingSituation = 101 THEN 'Emergency Shelter'\n",
    "                WHEN e.LivingSituation = 118 THEN 'Safe Haven'\n",
    "                WHEN e.LivingSituation = 215 THEN 'Foster Care Home'\n",
    "                WHEN e.LivingSituation = 206 THEN 'Hospital/ Medical Facility'\n",
    "                WHEN e.LivingSituation = 207 THEN 'Jail'\n",
    "                WHEN e.LivingSituation = 225 THEN 'Long-term care facility'\n",
    "                WHEN e.LivingSituation = 204 THEN 'Psychiatric hospital'\n",
    "                WHEN e.LivingSituation = 205 THEN 'Substance abuse treatment facility'\n",
    "                WHEN e.LivingSituation = 302 THEN 'Transitional Housing'\n",
    "                WHEN e.LivingSituation = 329 THEN 'Halfway House'\n",
    "                WHEN e.LivingSituation = 314 THEN 'Hotel/ Motel'\n",
    "                WHEN e.LivingSituation = 332 THEN 'Host Home'\n",
    "                WHEN e.LivingSituation = 312 THEN 'Staying or living with family, temporary tenure'\n",
    "                WHEN e.LivingSituation = 313 THEN 'Staying or living with friends, temporary tenure'\n",
    "                WHEN e.LivingSituation = 327 THEN 'HOPWA funded project TH'\n",
    "                WHEN e.LivingSituation = 336 THEN 'Staying/ living in friends house'\n",
    "                WHEN e.LivingSituation = 335 THEN 'Staying/ living in families house'\n",
    "                WHEN e.LivingSituation = 422 THEN 'Staying or living with family, permanent tenure'\n",
    "                WHEN e.LivingSituation = 423 THEN 'Staying or living with friends, permanent tenure'\n",
    "                WHEN e.LivingSituation = 426 THEN 'HOPWA funded project PH'\n",
    "                WHEN e.LivingSituation = 410 THEN 'Rental by client, no subsidy'\n",
    "                WHEN e.LivingSituation = 435 THEN 'Rental by client, with subsidy'\n",
    "                WHEN e.LivingSituation = 421 THEN 'Owned by client, no subsidy'\n",
    "                WHEN e.LivingSituation = 411 THEN 'Owned by client, with subsidy'\n",
    "                WHEN e.LivingSituation = 30  THEN 'No exit interview completed'\n",
    "                WHEN e.LivingSituation = 17  THEN 'Other'\n",
    "                WHEN e.LivingSituation = 24  THEN 'Deceased'\n",
    "                WHEN e.LivingSituation = 37  THEN 'Unable to determine'\n",
    "                WHEN e.LivingSituation = 8   THEN 'Client doesn''t know'\n",
    "                WHEN e.LivingSituation = 9   THEN 'Client prefers not to answer'\n",
    "                WHEN e.LivingSituation = 99  THEN 'Data Not Collected'\n",
    "                WHEN e.LivingSituation IS NULL THEN 'Data Not Collected'\n",
    "                ELSE 'Unknown'\n",
    "            END AS LivingSituation,\n",
    "            CASE\n",
    "                WHEN e.LivingSituation >= 100 AND e.LivingSituation < 200 THEN 'Homeless Situation'\n",
    "                WHEN e.LivingSituation >= 200 AND e.LivingSituation < 300 THEN 'Institutional Situation'\n",
    "                WHEN e.LivingSituation >= 300 AND e.LivingSituation < 400 THEN 'Temporary Situation'\n",
    "                WHEN e.LivingSituation >= 400 AND e.LivingSituation < 500 THEN 'Permanent Housing Situation'\n",
    "                ELSE 'Other'\n",
    "            END AS LivingSituationGrouping,\n",
    "            x.ExitID, x.ExitDate,\n",
    "            CASE\n",
    "                WHEN x.Destination IS NULL THEN 99\n",
    "                ELSE x.Destination\n",
    "            END AS Destination,\n",
    "            CASE\n",
    "                WHEN x.Destination >= 100 AND x.Destination < 200 THEN 'Homeless Situation'\n",
    "                WHEN x.Destination >= 200 AND x.Destination < 300 THEN 'Institutional Situation'\n",
    "                WHEN x.Destination >= 300 AND x.Destination < 400 THEN 'Temporary Situation'\n",
    "                WHEN x.Destination >= 400 AND x.Destination < 500 THEN 'Permanent Housing Situation'\n",
    "                ELSE 'Other'\n",
    "            END AS DestinationGrouping\n",
    "        FROM Enrollment e\n",
    "        INNER JOIN Exit x\n",
    "            ON e.EnrollmentID = x.EnrollmentID\n",
    "        \"\"\"\n",
    "        return query\n",
    "    \n",
    "    def log_experiment(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        hyperparameters: dict,\n",
    "        training_data_version: str,\n",
    "        performance_metrics: dict,\n",
    "        s3_model_location: str,\n",
    "        start_time: datetime,\n",
    "        end_time: datetime,\n",
    "        status: str,\n",
    "        notes: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs an experiment to the PostgreSQL database.\n",
    "\n",
    "        Parameters:\n",
    "        - model_type (str): The type of the model (e.g., 'RandomForest', 'GBM').\n",
    "        - hyperparameters (dict): A dictionary of hyperparameters used for the model.\n",
    "        - training_data_version (str): The version of the training data.\n",
    "        - performance_metrics (dict): A dictionary of performance metrics (e.g., accuracy, F1 score).\n",
    "        - s3_model_location (str): The S3 location where the model weights are stored.\n",
    "        - start_time (datetime): The start time of the experiment.\n",
    "        - end_time (datetime): The end time of the experiment.\n",
    "        - status (str): The status of the experiment (e.g., 'Completed', 'Failed').\n",
    "        - notes (str): Any additional notes for the experiment.\n",
    "\n",
    "        Returns:\n",
    "        - None: Logs the experiment in the database.\n",
    "        \"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO experiments (\n",
    "                model_type, \n",
    "                hyperparameters, \n",
    "                training_data_version, \n",
    "                performance_metrics, \n",
    "                s3_model_location, \n",
    "                start_time, \n",
    "                end_time, \n",
    "                status, \n",
    "                notes\n",
    "            ) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the insert query\n",
    "        cursor.execute(insert_query, (\n",
    "            model_type, \n",
    "            json.dumps(hyperparameters), \n",
    "            training_data_version, \n",
    "            json.dumps(performance_metrics), \n",
    "            s3_model_location, \n",
    "            start_time, \n",
    "            end_time, \n",
    "            status, \n",
    "            notes\n",
    "        ))\n",
    "\n",
    "        self.connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931a919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever:\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "        \n",
    "    def records(self):\n",
    "        query = self.data_source.enrollment_query()\n",
    "        total_count = self.data_source.count(query)\n",
    "\n",
    "        if total_count is None:\n",
    "            print(\"Failed to retrieve total count.\")\n",
    "            return None\n",
    "\n",
    "        all_data = pd.DataFrame()\n",
    "        batch_size = 10000\n",
    "        \n",
    "        try:\n",
    "            for offset in range(0, total_count, batch_size):\n",
    "                last_idx = min(offset + batch_size, total_count)\n",
    "                print(f\"Fetching {offset} to {last_idx} of {total_count}\")\n",
    "                data = self.data_source.data(query, limit=batch_size, offset=offset)\n",
    "                all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying data: {e}\")\n",
    "\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98876a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInspector:\n",
    "    def display(self, data, columns):\n",
    "        styled_df = data[[col.lower() for col in columns]].head().style.set_table_styles([{\n",
    "            'selector': 'table',\n",
    "            'props': [('max-width', '1000px'), ('overflow-x', 'scroll'), ('display', 'block')]\n",
    "        }])\n",
    "\n",
    "        display(styled_df)\n",
    "        print(f\"Total count: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d010f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3:\n",
    "    def __init__(self, bucket_name):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = boto3.client('s3')\n",
    "        \n",
    "    def save_model(self, model, name):\n",
    "        model_buffer = io.BytesIO()\n",
    "        joblib.dump(model, model_buffer)\n",
    "        model_buffer.seek(0)\n",
    "        try:\n",
    "            file_key = f\"models/{name}.joblib\"\n",
    "            self.s3.upload_fileobj(model_buffer, self.bucket_name, file_key)\n",
    "            print(f\"Model successfully uploaded to s3://{self.bucket_name}/{file_key}\")\n",
    "            return f\"s3://{self.bucket_name}/{file_key}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload model to S3: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def read_dataset(self, name):\n",
    "        base_key = f\"datasets/{name}\"\n",
    "        \n",
    "        # check for the last file written... hack\n",
    "        if not self.key_exists(f\"{base_key}/y_test\"):\n",
    "            return None\n",
    "        \n",
    "        dataset = Dataset()\n",
    "\n",
    "        dataset.X_train = self.read_dataset_df(f\"{base_key}/X_train\")\n",
    "        dataset.X_val   = self.read_dataset_df(f\"{base_key}/X_val\")\n",
    "        dataset.X_test  = self.read_dataset_df(f\"{base_key}/X_test\")\n",
    "        \n",
    "        # the label is a Series, so get the first column from the saved dataframe\n",
    "        dataset.y_train = self.read_dataset_df(f\"{base_key}/y_train\").iloc[:, 0]\n",
    "        dataset.y_val   = self.read_dataset_df(f\"{base_key}/y_val\").iloc[:, 0]\n",
    "        dataset.y_test  = self.read_dataset_df(f\"{base_key}/y_test\").iloc[:, 0]\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def read_dataset_df(self, file_key):\n",
    "        dataset_buffer = io.BytesIO()\n",
    "        self.s3.download_fileobj(self.bucket_name, file_key, dataset_buffer)\n",
    "        dataset_buffer.seek(0)\n",
    "        dataset_df = pd.read_parquet(dataset_buffer, engine='pyarrow')\n",
    "        \n",
    "        return dataset_df\n",
    "    \n",
    "    def save_dataset(self, dataset, name):\n",
    "        base_key = f\"datasets/{name}\"\n",
    "        \n",
    "        self.save_dataset_df(dataset.X_train, f\"{base_key}/X_train\")\n",
    "        self.save_dataset_df(dataset.X_val,   f\"{base_key}/X_val\")\n",
    "        self.save_dataset_df(dataset.X_test,  f\"{base_key}/X_test\")\n",
    "        \n",
    "        # the label is a Series, so save it as a dataframe for parquet\n",
    "        self.save_dataset_df(pd.DataFrame(dataset.y_train), f\"{base_key}/y_train\")\n",
    "        self.save_dataset_df(pd.DataFrame(dataset.y_val),   f\"{base_key}/y_val\")\n",
    "        self.save_dataset_df(pd.DataFrame(dataset.y_test),  f\"{base_key}/y_test\")\n",
    "        \n",
    "    def save_dataset_df(self, df, file_key):\n",
    "        dataset_buffer = io.BytesIO()\n",
    "        df.to_parquet(dataset_buffer, engine='pyarrow', index=False)\n",
    "\n",
    "        dataset_buffer.seek(0)\n",
    "        try:\n",
    "            self.s3.upload_fileobj(dataset_buffer, self.bucket_name, file_key)\n",
    "            print(f\"Dataset successfully uploaded to s3://{self.bucket_name}/{file_key}\")\n",
    "            return f\"s3://{self.bucket_name}/{file_key}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload dataset to S3: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def key_exists(self, file_key):\n",
    "        try:\n",
    "            self.s3.head_object(Bucket=self.bucket_name, Key=file_key)\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                return False\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81253035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def transformed_data(self):\n",
    "        self.transform()\n",
    "        return self.data\n",
    "    \n",
    "    def transform(self):\n",
    "        self.encode_categorical()\n",
    "        self.add_new_features()\n",
    "        \n",
    "    def encode_categorical(self):\n",
    "        le = LabelEncoder()\n",
    "        categorical_columns = ['livingsituation', 'livingsituationgrouping', 'destination', 'destinationgrouping']\n",
    "        for col in categorical_columns:\n",
    "            self.data[col] = le.fit_transform(self.data[col])\n",
    "            \n",
    "    def add_new_features(self):\n",
    "        # create a feature for enrollment duration in days\n",
    "        self.data['entrydate'] = pd.to_datetime(self.data['entrydate'])\n",
    "        self.data['exitdate'] = pd.to_datetime(self.data['exitdate'])\n",
    "        self.data['enrollment_duration'] = (self.data['exitdate'] - self.data['entrydate']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3be593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data=None, random_state=413):\n",
    "        self.random_state = random_state\n",
    "        self.data = data\n",
    "        if self.data is not None:\n",
    "            self.train_test_split()\n",
    "        \n",
    "    def train_test_split(self):\n",
    "        # create an 80/10/10 train/val/test split\n",
    "        self.X_train, X_temp, self.y_train, y_temp = train_test_split(\n",
    "            self.X(), self.y(), test_size=0.2, random_state=self.random_state\n",
    "        )\n",
    "        self.X_val, self.X_test, self.y_val, self.y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.5, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "    def X(self):\n",
    "        return self.data[['livingsituation', 'livingsituationgrouping', 'enrollment_duration']]\n",
    "    \n",
    "    def y(self):\n",
    "        return self.data['destinationgrouping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4cf5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel:\n",
    "    def __init__(self, config):\n",
    "        self.model = RandomForestClassifier(**config)\n",
    "        \n",
    "    def cross_validate(self, X_train, y_train, cv=5):\n",
    "        scores = cross_validate(self.model, X_train, y_train, cv=cv, scoring=['accuracy', 'f1'], return_train_score=False)\n",
    "        return {\n",
    "            \"cv_accuracy\": scores['test_accuracy'].mean(), \n",
    "            \"cv_f1_score\": scores['test_f1'].mean()\n",
    "        }\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4987e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningConfigurator:\n",
    "    def __init__(self, configs):\n",
    "        self.configs = self.make_configs(configs)\n",
    "        \n",
    "    def all_configs(self):\n",
    "        return self.configs\n",
    "    \n",
    "    def make_configs(self, configs):\n",
    "        keys = configs.keys()\n",
    "        values = configs.values()\n",
    "        combinations = list(itertools.product(*values))\n",
    "        param_combinations = [dict(zip(keys, combination)) for combination in combinations]\n",
    "        return param_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be1cd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = SimpleNamespace(**config)\n",
    "        \n",
    "    def run(self):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        data_source = DataSource(self.db_config())\n",
    "        s3 = S3(bucket_name='capstone-hmis')\n",
    "\n",
    "        dataset = s3.read_dataset(self.config.dataset_name)\n",
    "        if not dataset:\n",
    "            print(\"Querying RDS for data...\")\n",
    "            data_retriever = DataRetriever(data_source)\n",
    "            data = data_retriever.records()\n",
    "\n",
    "            print(\"Data before feature engineering\")\n",
    "            data_inspector = DataInspector()\n",
    "            data_inspector.display(data, data.columns.tolist())\n",
    "\n",
    "            feature_engineer = FeatureEngineer(data)\n",
    "            transformed_data = feature_engineer.transformed_data()\n",
    "            print(\"Data after feature engineering\")\n",
    "            data_inspector.display(transformed_data, transformed_data.columns.tolist())\n",
    "\n",
    "            dataset = Dataset(transformed_data)\n",
    "            s3.save_dataset(dataset, self.config.dataset_name)\n",
    "        else:\n",
    "            print(f\"Using saved Dataset {self.config.dataset_name}\")\n",
    "            \n",
    "        best_experiment = SimpleNamespace(accuracy=0, f1_score=0, model=None, uuid=None, config=None)\n",
    "        \n",
    "        # fine-tune the model by exploring all combinations of model params\n",
    "        configurator = FineTuningConfigurator(self.config.model_params)\n",
    "        for model_config in configurator.all_configs():\n",
    "            print(f\"\\nFitting {self.config.model_type} model with config: {model_config}\")\n",
    "            model = RandomForestModel(model_config)\n",
    "            model.fit(dataset.X_train, dataset.y_train)\n",
    "        \n",
    "            print(\"Evaluating model with validation set\")\n",
    "            y_pred = model.predict(dataset.X_val)\n",
    "\n",
    "            accuracy = accuracy_score(dataset.y_val, y_pred)\n",
    "            f1 = f1_score(dataset.y_val, y_pred, average='weighted')\n",
    "            print(\"\\n====================================================================\\n\")\n",
    "            print(f\" {self.config.model_type}({self.config.model_name}) Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "            print(\"\\n====================================================================\\n\")\n",
    "\n",
    "            experiment_uuid = str(uuid.uuid4())\n",
    "            \n",
    "            # determine if this was the best model by comparing f1 score\n",
    "            if f1 > best_experiment.f1_score:\n",
    "                best_experiment.accuracy = accuracy\n",
    "                best_experiment.f1_score = f1\n",
    "                best_experiment.model = model.model\n",
    "                best_experiment.uuid = experiment_uuid\n",
    "                best_experiment.config = model_config\n",
    "                \n",
    "            print(\"Logging experiment in RDS experiments table\")\n",
    "            data_source.log_experiment(\n",
    "                model_type=self.config.model_type,\n",
    "                hyperparameters=model.model.get_params(),\n",
    "                training_data_version=self.config.dataset_name,\n",
    "                performance_metrics={\"accuracy\": accuracy, \"f1_score\": f1},\n",
    "                s3_model_location=None,\n",
    "                start_time=start_time,\n",
    "                end_time=datetime.now(),\n",
    "                status='Completed',\n",
    "                notes=self.config.notes\n",
    "            )\n",
    "\n",
    "        print(\"\\n--------------------------------------------------------------------------\\n\")\n",
    "        print(f\" Best Experiment Config: {best_experiment.config}\")\n",
    "        print(f\" Accuracy: {best_experiment.accuracy:.4f}, F1: {best_experiment.f1_score:.4f}\")\n",
    "        print(\"\\n--------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        print(\"Saving best model to S3...\")\n",
    "        s3_model_location = s3.save_model(\n",
    "            model=best_experiment.model,\n",
    "            name=f\"{self.config.model_name}_{datetime.now():%Y-%m-%d}_{best_experiment.uuid}\"\n",
    "        )\n",
    "\n",
    "        print(\"Logging best experiment in RDS experiments table\")\n",
    "        data_source.log_experiment(\n",
    "            model_type=self.config.model_type,\n",
    "            hyperparameters=best_experiment.model.get_params(),\n",
    "            training_data_version=self.config.dataset_name,\n",
    "            performance_metrics={\"accuracy\": best_experiment.accuracy, \"f1_score\": best_experiment.f1_score},\n",
    "            s3_model_location=s3_model_location,\n",
    "            start_time=start_time,\n",
    "            end_time=datetime.now(),\n",
    "            status='Completed',\n",
    "            notes=self.config.notes\n",
    "        )\n",
    "\n",
    "        print(\"Experiment Complete!\")\n",
    "    \n",
    "    def db_config(self):\n",
    "        return SimpleNamespace(\n",
    "            host=\"capstone-database.cr62wyo4a7dt.us-east-2.rds.amazonaws.com\",\n",
    "            database=\"capstone\",\n",
    "            user=\"postgres\",\n",
    "            password=\"<PASSWORD_HERE>\",\n",
    "            port=\"5432\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd64a809",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to RDS successful\n",
      "Using saved Dataset v0.0.2\n",
      "\n",
      "Fitting RandomForest model with config: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Evaluating model with validation set\n",
      "\n",
      "====================================================================\n",
      "\n",
      " RandomForest(random-forest-v1) Accuracy: 0.7153, F1: 0.6228\n",
      "\n",
      "====================================================================\n",
      "\n",
      "Logging experiment in RDS experiments table\n",
      "\n",
      "Fitting RandomForest model with config: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Evaluating model with validation set\n",
      "\n",
      "====================================================================\n",
      "\n",
      " RandomForest(random-forest-v1) Accuracy: 0.7152, F1: 0.6221\n",
      "\n",
      "====================================================================\n",
      "\n",
      "Logging experiment in RDS experiments table\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      " Best Experiment Config: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " Accuracy: 0.7153, F1: 0.6228\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "Saving best model to S3...\n",
      "Model successfully uploaded to s3://capstone-hmis/models/random-forest-v1_2024-10-13_bf1877b5-40e2-490a-9538-8a62d59a08f7.joblib\n",
      "Logging best experiment in RDS experiments table\n",
      "Experiment Complete!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"model_type\": \"RandomForest\",\n",
    "    \"model_name\": \"random-forest-v1\",\n",
    "    \"model_params\": {\n",
    "        'n_estimators': [50, 100], # [50, 100, 200],\n",
    "        'max_depth': [10], # [10, 20, None],\n",
    "        'min_samples_split': [2], # [2, 5, 10],\n",
    "        'min_samples_leaf': [1], # [1, 2, 4],\n",
    "        'max_features': ['sqrt'], # ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \"dataset_name\": \"v0.0.2\",\n",
    "    \"notes\": \"First RandomForest experiment with fine-tuning\"\n",
    "}\n",
    "pipeline = DataPipeline(config)\n",
    "pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
